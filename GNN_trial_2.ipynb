{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /Users/ceragoguztuzun/miniconda3/envs/adversaGen/lib/python3.12/site-packages (4.2.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/ceragoguztuzun/miniconda3/envs/adversaGen/lib/python3.12/site-packages (from optuna) (1.15.1)\n",
      "Requirement already satisfied: colorlog in /Users/ceragoguztuzun/miniconda3/envs/adversaGen/lib/python3.12/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /Users/ceragoguztuzun/miniconda3/envs/adversaGen/lib/python3.12/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ceragoguztuzun/miniconda3/envs/adversaGen/lib/python3.12/site-packages (from optuna) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/ceragoguztuzun/miniconda3/envs/adversaGen/lib/python3.12/site-packages (from optuna) (2.0.39)\n",
      "Requirement already satisfied: tqdm in /Users/ceragoguztuzun/miniconda3/envs/adversaGen/lib/python3.12/site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in /Users/ceragoguztuzun/miniconda3/envs/adversaGen/lib/python3.12/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /Users/ceragoguztuzun/miniconda3/envs/adversaGen/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /Users/ceragoguztuzun/miniconda3/envs/adversaGen/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/ceragoguztuzun/miniconda3/envs/adversaGen/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from rdkit import RDLogger\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool, global_add_pool\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import random\n",
    "import warnings\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Add necessary imports\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "\n",
    "\n",
    "from torch_geometric.nn import GATv2Conv, GlobalAttention\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from sklearn.metrics import explained_variance_score, max_error, median_absolute_error\n",
    "\n",
    "\n",
    "# Suppress RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plot_dir_name = 'plots_by_c/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data analysis\n",
    "def analyze_data(df):\n",
    "    print(\"\\nData Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Plot Tg distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['tg'], bins=30, kde=True)\n",
    "    plt.title('Distribution of Glass Transition Temperatures (Tg)')\n",
    "    plt.xlabel('Tg (°C)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(f'{plot_dir_name}tg_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Analyze SMILES complexity\n",
    "    df['smiles_length'] = df['SMILES'].apply(len)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['smiles_length'], bins=30, kde=True)\n",
    "    plt.title('Distribution of SMILES String Lengths')\n",
    "    plt.xlabel('Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(f'{plot_dir_name}smiles_length_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Correlation between SMILES length and Tg\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x='smiles_length', y='tg', data=df, alpha=0.5)\n",
    "    plt.title('Relationship Between Molecule Complexity and Tg')\n",
    "    plt.xlabel('SMILES String Length')\n",
    "    plt.ylabel('Tg (°C)')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(f'{plot_dir_name}length_vs_tg.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_dataset(dataframe, smiles_col='SMILES', target_col='tg'):\n",
    "    data_list = []\n",
    "    valid_indices = []\n",
    "    invalid_smiles = []\n",
    "    \n",
    "    for idx, row in dataframe.iterrows():\n",
    "        smiles = row[smiles_col]\n",
    "        graph = smiles_to_graph(smiles)\n",
    "        if graph is not None:\n",
    "            # Add target value\n",
    "            graph.y = torch.tensor([row[target_col]], dtype=torch.float)\n",
    "            graph.smiles = smiles  # Store SMILES for reference\n",
    "            data_list.append(graph)\n",
    "            valid_indices.append(idx)\n",
    "        else:\n",
    "            invalid_smiles.append((idx, smiles))\n",
    "    \n",
    "    if invalid_smiles:\n",
    "        print(f\"\\nWarning: {len(invalid_smiles)} invalid SMILES strings found and skipped.\")\n",
    "        \n",
    "    return data_list, valid_indices\n",
    "\n",
    "\n",
    "# Convert SMILES to molecular graphs\n",
    "def smiles_to_graph(smiles):\n",
    "    # Convert SMILES to RDKit molecule\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    # Get atom features\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        # Atom features: One-hot encoding of atom type, formal charge, hybridization, aromaticity\n",
    "        atom_type_one_hot = [0] * 100  # Limit to first 100 elements\n",
    "        atom_num = atom.GetAtomicNum()\n",
    "        if atom_num < 100:\n",
    "            atom_type_one_hot[atom_num] = 1\n",
    "            \n",
    "        formal_charge = [atom.GetFormalCharge()]\n",
    "        hybridization_type = [0, 0, 0, 0, 0]  # One-hot encoding of hybridization\n",
    "        hyb_type = atom.GetHybridization()\n",
    "        if hyb_type == Chem.rdchem.HybridizationType.SP:\n",
    "            hybridization_type[0] = 1\n",
    "        elif hyb_type == Chem.rdchem.HybridizationType.SP2:\n",
    "            hybridization_type[1] = 1\n",
    "        elif hyb_type == Chem.rdchem.HybridizationType.SP3:\n",
    "            hybridization_type[2] = 1\n",
    "        elif hyb_type == Chem.rdchem.HybridizationType.SP3D:\n",
    "            hybridization_type[3] = 1\n",
    "        elif hyb_type == Chem.rdchem.HybridizationType.SP3D2:\n",
    "            hybridization_type[4] = 1\n",
    "            \n",
    "        is_aromatic = [1 if atom.GetIsAromatic() else 0]\n",
    "        degree = [atom.GetDegree()]\n",
    "        num_h = [atom.GetTotalNumHs()]\n",
    "        \n",
    "        # Combine all features\n",
    "        features = atom_type_one_hot + formal_charge + hybridization_type + is_aromatic + degree + num_h\n",
    "        atom_features.append(features)\n",
    "    \n",
    "    # Create node feature matrix (num_nodes x num_features)\n",
    "    x = torch.tensor(atom_features, dtype=torch.float)\n",
    "    \n",
    "    # Get edge indices (bonds)\n",
    "    edge_indices = []\n",
    "    edge_attr = []\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        \n",
    "        # Bond features\n",
    "        bond_type = bond.GetBondType()\n",
    "        bond_features = [0, 0, 0, 0]  # One-hot encoding of bond type\n",
    "        if bond_type == Chem.rdchem.BondType.SINGLE:\n",
    "            bond_features[0] = 1\n",
    "        elif bond_type == Chem.rdchem.BondType.DOUBLE:\n",
    "            bond_features[1] = 1\n",
    "        elif bond_type == Chem.rdchem.BondType.TRIPLE:\n",
    "            bond_features[2] = 1\n",
    "        elif bond_type == Chem.rdchem.BondType.AROMATIC:\n",
    "            bond_features[3] = 1\n",
    "            \n",
    "        is_conjugated = [1 if bond.GetIsConjugated() else 0]\n",
    "        is_in_ring = [1 if bond.IsInRing() else 0]\n",
    "        \n",
    "        # Combine all features\n",
    "        features = bond_features + is_conjugated + is_in_ring\n",
    "        \n",
    "        # Add bonds in both directions\n",
    "        edge_indices.append([i, j])\n",
    "        edge_indices.append([j, i])\n",
    "        edge_attr.append(features)\n",
    "        edge_attr.append(features)\n",
    "    \n",
    "    if len(edge_indices) == 0:  # For molecules with no bonds\n",
    "        edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.zeros((0, 6), dtype=torch.float)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, smiles=smiles)\n",
    "\n",
    "\n",
    "# If the above approach has issues with coordinates, here's an even simpler alternative\n",
    "def visualize_molecule_with_color_overlay(smiles, atom_importances, tg, idx, prediction=None, plot_dir_name=\"\"):\n",
    "    \"\"\"\n",
    "    Visualize a molecule with atom importance by creating two overlaid images:\n",
    "    1. A standard black and white molecule with atom indices\n",
    "    2. Colored circles positioned approximately where atoms are\n",
    "    \n",
    "    Parameters are the same as previous function.\n",
    "    \"\"\"\n",
    "    # Convert SMILES to molecule\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        print(f\"Could not convert SMILES to molecule: {smiles}\")\n",
    "        return\n",
    "    \n",
    "    # Normalize importance values\n",
    "    if len(atom_importances) > 0 and max(atom_importances) > 0:\n",
    "        norm_importances = atom_importances / max(atom_importances)\n",
    "    else:\n",
    "        norm_importances = atom_importances\n",
    "    \n",
    "    # Set up the colormap\n",
    "    cmap = plt.cm.coolwarm\n",
    "    \n",
    "    # Generate a standard molecule image with atom indices\n",
    "    drawer = rdMolDraw2D.MolDraw2DCairo(800, 800)\n",
    "    drawer.drawOptions().addAtomIndices = True\n",
    "    drawer.DrawMolecule(mol)\n",
    "    drawer.FinishDrawing()\n",
    "    png_data = drawer.GetDrawingText()\n",
    "    molecule_img = Image.open(BytesIO(png_data))\n",
    "    \n",
    "    # Create a figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    \n",
    "    # Display the molecule image\n",
    "    ax.imshow(molecule_img)\n",
    "    ax.set_title(f\"SMILES: {smiles}\\nTg: {tg}°C\\nPredicted Tg: {prediction:.2f}°C\", fontsize=14)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(0, 1))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, shrink=0.7, pad=0.05, orientation='horizontal')\n",
    "    cbar.set_label('Atom Importance', fontsize=14)\n",
    "    cbar.set_ticks([0.0, 0.5, 1.0])\n",
    "    cbar.set_ticklabels(['Low', 'Medium', 'High'])\n",
    "    \n",
    "    \n",
    "    plt.figtext(0.5, 0.01, \"Blue = Low Importance, Red = High Importance for Tg Prediction\",\n",
    "                ha=\"center\", fontsize=12, \n",
    "                bbox={\"facecolor\":\"lightgray\", \"alpha\":0.3, \"pad\":5})\n",
    "    \n",
    "    plt.savefig(f'{plot_dir_name}molecule_attention_{idx}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Also print to console\n",
    "    top_indices = np.argsort(atom_importances)[-5:][::-1]\n",
    "    print(f\"Top 5 important atoms for molecule {idx} (indices): {top_indices}\")\n",
    "    print(f\"Importance values: {atom_importances[top_indices]}\")\n",
    "\n",
    "\n",
    "# Function to analyze model interpretation\n",
    "def analyze_model_interpretation(model, data_loader, device, num_examples=5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of examples\n",
    "    examples = []\n",
    "    predictions = []\n",
    "    for data in data_loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            \n",
    "        for i in range(min(len(data.y), num_examples - len(examples))):\n",
    "            if len(examples) >= num_examples:\n",
    "                break\n",
    "                \n",
    "            # Extract single molecule\n",
    "            single_data = data[i].to(device)\n",
    "            \n",
    "            # Get atom importance\n",
    "            atom_importance = model.get_atom_importance(single_data)\n",
    "            \n",
    "            # Store example and prediction\n",
    "            examples.append((single_data.smiles, atom_importance, single_data.y.item()))\n",
    "            predictions.append(output[i].item())\n",
    "            \n",
    "        if len(examples) >= num_examples:\n",
    "            break\n",
    "    \n",
    "    # Visualize examples with attention weights\n",
    "    print(\"\\nVisualizing molecules with attention weights...\")\n",
    "    for i, (smiles, atom_importance, tg) in enumerate(examples):\n",
    "        #visualize_molecule_with_attention\n",
    "        visualize_molecule_with_color_overlay(smiles, atom_importance, tg, i, predictions[i])\n",
    "        \n",
    "        # Print most important atoms\n",
    "        top_indices = np.argsort(atom_importance)[-5:][::-1]\n",
    "        print(f\"\\nMolecule {i+1} (SMILES: {smiles})\")\n",
    "        print(f\"Actual Tg: {tg:.2f}°C, Predicted Tg: {predictions[i]:.2f}°C\")\n",
    "        print(f\"Top 5 important atoms (indices): {top_indices}\")\n",
    "        print(f\"Importance values: {atom_importance[top_indices]}\")\n",
    "\n",
    "\n",
    "# Early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "        elif val_loss > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "    \n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            predictions.extend(output.cpu().numpy())\n",
    "            actual.extend(data.y.cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    actual = np.array(actual)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actual, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predictions)\n",
    "    r2 = r2_score(actual, predictions)\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'actual': actual,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the GAT Model with attention\n",
    "class GATTgPredictor(torch.nn.Module):\n",
    "    def __init__(self, node_features, edge_features, hidden_channels=64, heads=4):\n",
    "        super(GATTgPredictor, self).__init__()\n",
    "        \n",
    "        # Graph attention layers - these will provide interpretability\n",
    "        self.conv1 = GATConv(node_features, hidden_channels, heads=heads, dropout=0.2)\n",
    "        self.conv2 = GATConv(hidden_channels*heads, hidden_channels, heads=heads, dropout=0.2)\n",
    "        self.conv3 = GATConv(hidden_channels*heads, hidden_channels, heads=1, dropout=0.2)\n",
    "        \n",
    "        # Batch normalization for stability\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels*heads)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels*heads)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        # Fully connected layers for regression\n",
    "        self.fc1 = torch.nn.Linear(hidden_channels, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 1)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        \n",
    "        # For storing attention weights\n",
    "        self.attention_weights = None\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # First GAT layer with attention\n",
    "        x1, attention_weights1 = self.conv1(x, edge_index, return_attention_weights=True)\n",
    "        x1 = F.relu(self.bn1(x1))\n",
    "        x1 = self.dropout(x1)\n",
    "        \n",
    "        # Second GAT layer\n",
    "        x2, attention_weights2 = self.conv2(x1, edge_index, return_attention_weights=True)\n",
    "        x2 = F.relu(self.bn2(x2))\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        # Third GAT layer - final layer for capturing node importance\n",
    "        x3, attention_weights3 = self.conv3(x2, edge_index, return_attention_weights=True)\n",
    "        x3 = F.relu(self.bn3(x3))\n",
    "        \n",
    "        # Store attention weights from the final layer for interpretation\n",
    "        # edge_index, attention (edge_index shape: [2, num_edges], attention shape: [num_edges, heads])\n",
    "        self.attention_weights = attention_weights3\n",
    "        \n",
    "        # Global pooling - aggregate node features for each graph\n",
    "        x = global_mean_pool(x3, batch)\n",
    "        \n",
    "        # Apply fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x.view(-1)\n",
    "    \n",
    "    # Method to get atom-level importance\n",
    "    def get_atom_importance(self, data):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass to get attention weights\n",
    "            _ = self(data)\n",
    "            \n",
    "            # Extract attention weights\n",
    "            edge_index, attn_weights = self.attention_weights\n",
    "            \n",
    "            # Initialize importance scores for each atom\n",
    "            num_nodes = data.x.size(0)\n",
    "            importance = torch.zeros(num_nodes, device=data.x.device)\n",
    "            \n",
    "            # Sum attention weights for each node\n",
    "            for i in range(edge_index.size(1)):\n",
    "                target_node = edge_index[1, i].item()\n",
    "                importance[target_node] += attn_weights[i].item()\n",
    "            \n",
    "            # Normalize importance scores\n",
    "            if importance.max() > 0:\n",
    "                importance = importance / importance.max()\n",
    "                \n",
    "            return importance.cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (7174, 2)\n",
      "        SMILES    tg\n",
      "0          *C* -54.0\n",
      "1      *CC(*)C  -3.0\n",
      "2     *CC(*)CC -24.1\n",
      "3    *CC(*)CCC -37.0\n",
      "4  *CC(*)C(C)C  60.0\n",
      "\n",
      "Data Statistics:\n",
      "                tg\n",
      "count  7174.000000\n",
      "mean    141.948090\n",
      "std     112.178143\n",
      "min    -139.000000\n",
      "25%      55.000000\n",
      "50%     134.000000\n",
      "75%     231.000000\n",
      "max     495.000000\n",
      "\n",
      "Preparing dataset...\n",
      "Valid molecules processed: 7174 out of 7174\n",
      "Node features: 109, Edge features: 6\n",
      "Using device: cpu\n",
      "\n",
      "Performing cross-validation with baseline model...\n",
      "\n",
      "Performing 10-fold cross-validation...\n",
      "\n",
      "Fold 1/10\n",
      "Epoch 1/100, Train Loss: 31367.9359, Val Loss: 28152.6543, Val R²: -1.2918\n",
      "Epoch 10/100, Train Loss: 3567.9802, Val Loss: 3875.6616, Val R²: 0.6845\n",
      "Epoch 20/100, Train Loss: 3293.0040, Val Loss: 3137.3389, Val R²: 0.7446\n",
      "Epoch 30/100, Train Loss: 3082.8440, Val Loss: 3093.8530, Val R²: 0.7481\n",
      "Early stopping at epoch 34\n",
      "Fold 1 Results - Val MSE: 2946.2444, Val RMSE: 54.2793, Val R²: 0.7602\n",
      "\n",
      "Fold 2/10\n",
      "Epoch 1/100, Train Loss: 31292.4597, Val Loss: 26808.6523, Val R²: -1.1599\n",
      "Epoch 10/100, Train Loss: 3720.6119, Val Loss: 2873.3220, Val R²: 0.7685\n",
      "Epoch 20/100, Train Loss: 3389.7224, Val Loss: 2522.7161, Val R²: 0.7968\n",
      "Epoch 30/100, Train Loss: 3181.5184, Val Loss: 2278.7341, Val R²: 0.8164\n",
      "Epoch 40/100, Train Loss: 3155.7764, Val Loss: 2396.1697, Val R²: 0.8069\n",
      "Epoch 50/100, Train Loss: 2995.8538, Val Loss: 2279.1675, Val R²: 0.8164\n",
      "Epoch 60/100, Train Loss: 2973.3677, Val Loss: 2255.9404, Val R²: 0.8182\n",
      "Early stopping at epoch 67\n",
      "Fold 2 Results - Val MSE: 2077.9373, Val RMSE: 45.5844, Val R²: 0.8326\n",
      "\n",
      "Fold 3/10\n",
      "Epoch 1/100, Train Loss: 31128.1252, Val Loss: 28820.9551, Val R²: -1.3573\n",
      "Epoch 10/100, Train Loss: 3712.8810, Val Loss: 2531.3462, Val R²: 0.7930\n",
      "Epoch 20/100, Train Loss: 3474.6661, Val Loss: 2246.2939, Val R²: 0.8163\n",
      "Epoch 30/100, Train Loss: 3225.1878, Val Loss: 2700.1233, Val R²: 0.7792\n",
      "Early stopping at epoch 36\n",
      "Fold 3 Results - Val MSE: 2566.9258, Val RMSE: 50.6648, Val R²: 0.7900\n",
      "\n",
      "Fold 4/10\n",
      "Epoch 1/100, Train Loss: 30801.9670, Val Loss: 24290.1113, Val R²: -0.9729\n",
      "Epoch 10/100, Train Loss: 3509.6891, Val Loss: 2782.4546, Val R²: 0.7740\n",
      "Epoch 20/100, Train Loss: 3264.8360, Val Loss: 2622.7466, Val R²: 0.7870\n",
      "Epoch 30/100, Train Loss: 3171.2253, Val Loss: 2594.4688, Val R²: 0.7893\n",
      "Epoch 40/100, Train Loss: 2981.0352, Val Loss: 2285.2649, Val R²: 0.8144\n",
      "Epoch 50/100, Train Loss: 2979.4000, Val Loss: 2316.8650, Val R²: 0.8118\n",
      "Epoch 60/100, Train Loss: 2750.9458, Val Loss: 2184.8350, Val R²: 0.8225\n",
      "Epoch 70/100, Train Loss: 2751.6802, Val Loss: 2215.2646, Val R²: 0.8201\n",
      "Epoch 80/100, Train Loss: 2702.2478, Val Loss: 2118.6240, Val R²: 0.8279\n",
      "Epoch 90/100, Train Loss: 2641.0902, Val Loss: 2040.3203, Val R²: 0.8343\n",
      "Epoch 100/100, Train Loss: 2632.6541, Val Loss: 2054.5430, Val R²: 0.8331\n",
      "Fold 4 Results - Val MSE: 2054.5430, Val RMSE: 45.3271, Val R²: 0.8331\n",
      "\n",
      "Fold 5/10\n",
      "Epoch 1/100, Train Loss: 31922.6605, Val Loss: 28414.4570, Val R²: -1.2913\n",
      "Epoch 10/100, Train Loss: 3759.8924, Val Loss: 3355.3552, Val R²: 0.7294\n",
      "Epoch 20/100, Train Loss: 3422.4521, Val Loss: 2935.8325, Val R²: 0.7633\n",
      "Epoch 30/100, Train Loss: 3230.4925, Val Loss: 2772.4766, Val R²: 0.7764\n",
      "Epoch 40/100, Train Loss: 3192.9688, Val Loss: 2380.0747, Val R²: 0.8081\n",
      "Epoch 50/100, Train Loss: 3001.7020, Val Loss: 2392.6174, Val R²: 0.8071\n",
      "Epoch 60/100, Train Loss: 2893.6200, Val Loss: 2430.1638, Val R²: 0.8040\n",
      "Early stopping at epoch 64\n",
      "Fold 5 Results - Val MSE: 2514.8943, Val RMSE: 50.1487, Val R²: 0.7972\n",
      "\n",
      "Fold 6/10\n",
      "Epoch 1/100, Train Loss: 30493.7850, Val Loss: 27516.6914, Val R²: -1.0588\n",
      "Epoch 10/100, Train Loss: 3552.2082, Val Loss: 4066.1931, Val R²: 0.6958\n",
      "Epoch 20/100, Train Loss: 3407.8736, Val Loss: 2704.4819, Val R²: 0.7977\n",
      "Epoch 30/100, Train Loss: 3169.8255, Val Loss: 2544.1045, Val R²: 0.8097\n",
      "Epoch 40/100, Train Loss: 3075.1102, Val Loss: 2364.3713, Val R²: 0.8231\n",
      "Epoch 50/100, Train Loss: 2790.2609, Val Loss: 2319.3340, Val R²: 0.8265\n",
      "Epoch 60/100, Train Loss: 2807.6098, Val Loss: 2276.6440, Val R²: 0.8297\n",
      "Epoch 70/100, Train Loss: 2652.4122, Val Loss: 2134.4031, Val R²: 0.8403\n",
      "Epoch 80/100, Train Loss: 2754.4398, Val Loss: 2102.4204, Val R²: 0.8427\n",
      "Epoch 90/100, Train Loss: 2668.8708, Val Loss: 2094.8035, Val R²: 0.8433\n",
      "Early stopping at epoch 94\n",
      "Fold 6 Results - Val MSE: 2085.5093, Val RMSE: 45.6674, Val R²: 0.8440\n",
      "\n",
      "Fold 7/10\n",
      "Epoch 1/100, Train Loss: 30611.5480, Val Loss: 25722.1777, Val R²: -1.0887\n",
      "Epoch 10/100, Train Loss: 3553.5390, Val Loss: 3003.7297, Val R²: 0.7561\n",
      "Epoch 20/100, Train Loss: 3266.9723, Val Loss: 3152.8745, Val R²: 0.7440\n",
      "Epoch 30/100, Train Loss: 3080.5968, Val Loss: 2796.0544, Val R²: 0.7730\n",
      "Epoch 40/100, Train Loss: 2959.1864, Val Loss: 2636.5413, Val R²: 0.7859\n",
      "Epoch 50/100, Train Loss: 2835.2559, Val Loss: 2936.7402, Val R²: 0.7615\n",
      "Epoch 60/100, Train Loss: 2845.8600, Val Loss: 2643.9287, Val R²: 0.7853\n",
      "Epoch 70/100, Train Loss: 2757.0346, Val Loss: 2597.6108, Val R²: 0.7891\n",
      "Early stopping at epoch 72\n",
      "Fold 7 Results - Val MSE: 2590.1528, Val RMSE: 50.8935, Val R²: 0.7897\n",
      "\n",
      "Fold 8/10\n",
      "Epoch 1/100, Train Loss: 31308.7522, Val Loss: 26512.0000, Val R²: -1.1085\n",
      "Epoch 10/100, Train Loss: 3569.7767, Val Loss: 3420.2197, Val R²: 0.7280\n",
      "Epoch 20/100, Train Loss: 3277.3251, Val Loss: 2823.7578, Val R²: 0.7754\n",
      "Epoch 30/100, Train Loss: 3161.5005, Val Loss: 2373.4707, Val R²: 0.8112\n",
      "Epoch 40/100, Train Loss: 3049.5823, Val Loss: 2355.0544, Val R²: 0.8127\n",
      "Epoch 50/100, Train Loss: 2932.0509, Val Loss: 2202.4016, Val R²: 0.8248\n",
      "Epoch 60/100, Train Loss: 2912.8076, Val Loss: 2162.7703, Val R²: 0.8280\n",
      "Epoch 70/100, Train Loss: 2856.2957, Val Loss: 2228.4258, Val R²: 0.8228\n",
      "Epoch 80/100, Train Loss: 2960.8034, Val Loss: 2218.7834, Val R²: 0.8235\n",
      "Early stopping at epoch 87\n",
      "Fold 8 Results - Val MSE: 2164.8481, Val RMSE: 46.5279, Val R²: 0.8278\n",
      "\n",
      "Fold 9/10\n",
      "Epoch 1/100, Train Loss: 30402.9523, Val Loss: 25387.8711, Val R²: -0.9980\n",
      "Epoch 10/100, Train Loss: 3358.5947, Val Loss: 3641.0425, Val R²: 0.7134\n",
      "Epoch 20/100, Train Loss: 3150.5166, Val Loss: 3542.0903, Val R²: 0.7212\n",
      "Epoch 30/100, Train Loss: 3063.3358, Val Loss: 2788.8816, Val R²: 0.7805\n",
      "Epoch 40/100, Train Loss: 2928.1453, Val Loss: 2645.6699, Val R²: 0.7918\n",
      "Epoch 50/100, Train Loss: 2786.7085, Val Loss: 2554.3079, Val R²: 0.7990\n",
      "Epoch 60/100, Train Loss: 2762.2324, Val Loss: 2516.1106, Val R²: 0.8020\n",
      "Epoch 70/100, Train Loss: 2631.6665, Val Loss: 2438.6289, Val R²: 0.8081\n",
      "Epoch 80/100, Train Loss: 2632.8512, Val Loss: 2352.2573, Val R²: 0.8149\n",
      "Epoch 90/100, Train Loss: 2537.3282, Val Loss: 2332.4495, Val R²: 0.8164\n",
      "Epoch 100/100, Train Loss: 2394.9537, Val Loss: 2327.4192, Val R²: 0.8168\n",
      "Fold 9 Results - Val MSE: 2327.4192, Val RMSE: 48.2433, Val R²: 0.8168\n",
      "\n",
      "Fold 10/10\n",
      "Epoch 1/100, Train Loss: 30744.2288, Val Loss: 28359.7656, Val R²: -1.1621\n",
      "Epoch 10/100, Train Loss: 3432.4240, Val Loss: 2770.5771, Val R²: 0.7888\n",
      "Epoch 20/100, Train Loss: 3230.4271, Val Loss: 2582.2986, Val R²: 0.8031\n",
      "Epoch 30/100, Train Loss: 3165.7186, Val Loss: 2642.2139, Val R²: 0.7986\n",
      "Epoch 40/100, Train Loss: 3038.0302, Val Loss: 2352.3467, Val R²: 0.8207\n",
      "Epoch 50/100, Train Loss: 2906.2652, Val Loss: 2274.5283, Val R²: 0.8266\n",
      "Epoch 60/100, Train Loss: 2762.9512, Val Loss: 2165.1887, Val R²: 0.8349\n",
      "Epoch 70/100, Train Loss: 2821.9979, Val Loss: 2253.3687, Val R²: 0.8282\n",
      "Epoch 80/100, Train Loss: 2739.3226, Val Loss: 2290.6697, Val R²: 0.8254\n",
      "Epoch 90/100, Train Loss: 2806.0410, Val Loss: 2120.7075, Val R²: 0.8383\n",
      "Epoch 100/100, Train Loss: 2741.1381, Val Loss: 2187.3552, Val R²: 0.8332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 16:27:52,739] A new study created in memory with name: no-name-1096ad54-5878-4c52-8776-4b14717ff0ca\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10 Results - Val MSE: 2187.3552, Val RMSE: 46.7692, Val R²: 0.8332\n",
      "Average train_loss: 2791.7264 ± 210.4418\n",
      "Average val_loss: 2351.5829 ± 279.2697\n",
      "Average val_r2: 0.8125 ± 0.0255\n",
      "Average val_rmse: 48.4106 ± 2.8284\n",
      "Average val_mae: 35.4905 ± 2.7705\n",
      "\n",
      "Optimizing hyperparameters...\n",
      "\n",
      "Optimizing hyperparameters with Optuna...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 16:33:53,584] Trial 0 finished with value: 0.7942336797714233 and parameters: {'hidden_channels': 64, 'heads': 8, 'dropout': 0.39279757672456206, 'learning_rate': 0.0015751320499779737, 'weight_decay': 2.9380279387035354e-06, 'batch_size': 64, 'r2_weight': 0.7080725777960455}. Best is trial 0 with value: 0.7942336797714233.\n",
      "[I 2025-03-16 16:38:40,858] Trial 1 finished with value: 0.7383342981338501 and parameters: {'hidden_channels': 32, 'heads': 8, 'dropout': 0.4329770563201687, 'learning_rate': 0.00026587543983272726, 'weight_decay': 3.5113563139704077e-06, 'batch_size': 64, 'r2_weight': 0.2912291401980419}. Best is trial 0 with value: 0.7942336797714233.\n",
      "[I 2025-03-16 16:43:31,287] Trial 2 finished with value: 0.8116323947906494 and parameters: {'hidden_channels': 96, 'heads': 2, 'dropout': 0.21685785941408728, 'learning_rate': 0.0005404103854647331, 'weight_decay': 2.334586407601622e-05, 'batch_size': 16, 'r2_weight': 0.046450412719997725}. Best is trial 2 with value: 0.8116323947906494.\n",
      "[I 2025-03-16 16:44:38,553] Trial 3 finished with value: 0.6884735226631165 and parameters: {'hidden_channels': 96, 'heads': 2, 'dropout': 0.12602063719411183, 'learning_rate': 0.007902619549708232, 'weight_decay': 0.0007886714129990489, 'batch_size': 16, 'r2_weight': 0.4401524937396013}. Best is trial 2 with value: 0.8116323947906494.\n",
      "[I 2025-03-16 16:47:10,570] Trial 4 finished with value: 0.8263098001480103 and parameters: {'hidden_channels': 32, 'heads': 4, 'dropout': 0.11375540844608736, 'learning_rate': 0.006586289317583112, 'weight_decay': 5.975027999960295e-06, 'batch_size': 16, 'r2_weight': 0.18485445552552704}. Best is trial 4 with value: 0.8263098001480103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial:\n",
      "  Value (R²): 0.8263\n",
      "  Hyperparameters:\n",
      "    hidden_channels: 32\n",
      "    heads: 4\n",
      "    dropout: 0.11375540844608736\n",
      "    learning_rate: 0.006586289317583112\n",
      "    weight_decay: 5.975027999960295e-06\n",
      "    batch_size: 16\n",
      "    r2_weight: 0.18485445552552704\n",
      "\n",
      "Training final model with optimized parameters...\n",
      "\n",
      "Running final experiment with optimized parameters...\n",
      "Training set: 5739, Test set: 1435\n",
      "\n",
      "Training optimized model...\n",
      "Epoch 1/200, Train Loss: 8368.1025, Train R²: 0.7229\n",
      "Epoch 10/200, Train Loss: 3302.9197, Train R²: 0.7981\n",
      "Epoch 20/200, Train Loss: 3227.9120, Train R²: 0.7772\n",
      "Epoch 30/200, Train Loss: 3069.7076, Train R²: 0.7412\n",
      "Epoch 40/200, Train Loss: 2958.4408, Train R²: 0.8130\n",
      "Epoch 50/200, Train Loss: 2609.1094, Train R²: 0.8498\n",
      "Epoch 60/200, Train Loss: 2500.7507, Train R²: 0.8197\n",
      "Epoch 70/200, Train Loss: 2416.1790, Train R²: 0.8551\n",
      "Epoch 80/200, Train Loss: 2390.9546, Train R²: 0.8563\n",
      "Epoch 90/200, Train Loss: 2305.0630, Train R²: 0.8725\n",
      "Epoch 100/200, Train Loss: 2182.7518, Train R²: 0.8604\n",
      "Epoch 110/200, Train Loss: 2075.5309, Train R²: 0.8708\n",
      "Epoch 120/200, Train Loss: 2035.0690, Train R²: 0.8764\n",
      "Epoch 130/200, Train Loss: 2055.9100, Train R²: 0.8699\n",
      "Epoch 140/200, Train Loss: 1991.3058, Train R²: 0.8766\n",
      "Epoch 150/200, Train Loss: 2054.1418, Train R²: 0.8815\n",
      "Epoch 160/200, Train Loss: 1972.2823, Train R²: 0.8815\n",
      "Epoch 170/200, Train Loss: 2019.1670, Train R²: 0.8768\n",
      "Early stopping at epoch 178\n",
      "\n",
      "Test Results with Optimized Model:\n",
      "MSE: 1869.2415\n",
      "RMSE: 43.2347\n",
      "MAE: 31.0287\n",
      "R²: 0.8487\n",
      "\n",
      "Analyzing model interpretation with attention weights...\n",
      "\n",
      "Visualizing molecules with attention weights...\n",
      "Top 5 important atoms for molecule 0 (indices): [38 11 14 55 16]\n",
      "Importance values: [1.        1.        1.        0.9999999 0.9999999]\n",
      "\n",
      "Molecule 1 (SMILES: *Oc1ccc(NC(=O)Nc2cc(NC(=O)Nc3ccc(*)c(-c4ccc(Oc5ccccc5)cc4)c3)ccc2C)cc1-c1ccc(Oc2ccccc2)cc1)\n",
      "Actual Tg: 279.00°C, Predicted Tg: 165.18°C\n",
      "Top 5 important atoms (indices): [38 11 14 55 16]\n",
      "Importance values: [1.        1.        1.        0.9999999 0.9999999]\n",
      "Top 5 important atoms for molecule 1 (indices): [24 23  1  2  4]\n",
      "Importance values: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Molecule 2 (SMILES: *CCCCCCCCCCNC(=O)CCP(C)(=O)CCC(=O)N*)\n",
      "Actual Tg: 45.50°C, Predicted Tg: 53.35°C\n",
      "Top 5 important atoms (indices): [24 23  1  2  4]\n",
      "Importance values: [1. 1. 1. 1. 1.]\n",
      "Top 5 important atoms for molecule 2 (indices): [ 4 10  8  5  3]\n",
      "Importance values: [1.        0.9999999 0.9999999 0.9999999 0.9999999]\n",
      "\n",
      "Molecule 3 (SMILES: *CC(*)c1ccc(F)cc1)\n",
      "Actual Tg: 95.00°C, Predicted Tg: 77.12°C\n",
      "Top 5 important atoms (indices): [ 4 10  8  5  3]\n",
      "Importance values: [1.        0.9999999 0.9999999 0.9999999 0.9999999]\n",
      "Top 5 important atoms for molecule 3 (indices): [39 20 18 17 16]\n",
      "Importance values: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Molecule 4 (SMILES: *C(=O)c1ccc2c(c1)C(=O)N(c1ccc(C(=O)c3ccc(N4C(=O)c5ccc(*)cc5C4=O)cc3)cc1)C2=O)\n",
      "Actual Tg: 294.00°C, Predicted Tg: 267.62°C\n",
      "Top 5 important atoms (indices): [39 20 18 17 16]\n",
      "Importance values: [1. 1. 1. 1. 1.]\n",
      "Top 5 important atoms for molecule 4 (indices): [17 16  1  2  3]\n",
      "Importance values: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Molecule 5 (SMILES: *N=P(*)(Oc1ccccc1)Oc1ccccc1)\n",
      "Actual Tg: -8.00°C, Predicted Tg: -19.71°C\n",
      "Top 5 important atoms (indices): [17 16  1  2  3]\n",
      "Importance values: [1. 1. 1. 1. 1.]\n",
      "Top 5 important atoms for molecule 5 (indices): [14 23 22  1  2]\n",
      "Importance values: [1.        0.9999999 0.9999999 0.9999999 0.9999999]\n",
      "\n",
      "Molecule 6 (SMILES: *CC(C(C)C)C(COC(=O)c1ccc(C(=O)O*)cc1)C(C)C)\n",
      "Actual Tg: 65.00°C, Predicted Tg: 73.44°C\n",
      "Top 5 important atoms (indices): [14 23 22  1  2]\n",
      "Importance values: [1.        0.9999999 0.9999999 0.9999999 0.9999999]\n",
      "Top 5 important atoms for molecule 6 (indices): [13 72 87 19 92]\n",
      "Importance values: [1.        1.        1.        1.        0.9999999]\n",
      "\n",
      "Molecule 7 (SMILES: *c1ccc(NC2CC(=O)N(c3ccc(-c4sc(-c5ccc(N6C(=O)CC(Nc7ccc(-c8nc(-c9ccc(-c%10nc(-c%11ccc([N+](=O)[O-])cc%11)c(-c%11ccc([N+](=O)[O-])cc%11)[nH]%10)cc9)[nH]c8*)cc7)C6=O)cc5)c(-c5ccccc5)c4-c4ccccc4)cc3)C2=O)cc1)\n",
      "Actual Tg: 152.00°C, Predicted Tg: 313.00°C\n",
      "Top 5 important atoms (indices): [13 72 87 19 92]\n",
      "Importance values: [1.        1.        1.        1.        0.9999999]\n",
      "Top 5 important atoms for molecule 7 (indices): [11  9 22  1  2]\n",
      "Importance values: [1.        1.        0.9999999 0.9999999 0.9999999]\n",
      "\n",
      "Molecule 8 (SMILES: *Cc1cc(C)c(CNC(=O)c2ccc(C(=O)N*)cc2)cc1C)\n",
      "Actual Tg: 225.00°C, Predicted Tg: 223.88°C\n",
      "Top 5 important atoms (indices): [11  9 22  1  2]\n",
      "Importance values: [1.        1.        0.9999999 0.9999999 0.9999999]\n",
      "Top 5 important atoms for molecule 8 (indices): [24 15 43 39 36]\n",
      "Importance values: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Molecule 9 (SMILES: *CCN(CCOC(=O)Nc1ccc(C)c(NC(=O)O*)c1)c1ccc(/N=N/c2ccc(-c3nc4ccccc4o3)cc2)cc1)\n",
      "Actual Tg: 152.00°C, Predicted Tg: 131.23°C\n",
      "Top 5 important atoms (indices): [24 15 43 39 36]\n",
      "Importance values: [1. 1. 1. 1. 1.]\n",
      "Top 5 important atoms for molecule 9 (indices): [10  1  4 22  2]\n",
      "Importance values: [1.        1.        1.        0.9999999 0.9999999]\n",
      "\n",
      "Molecule 10 (SMILES: *CCCC(CCNC(=O)c1ccc(C(=O)N*)cc1)C(C)C)\n",
      "Actual Tg: 150.00°C, Predicted Tg: 142.69°C\n",
      "Top 5 important atoms (indices): [10  1  4 22  2]\n",
      "Importance values: [1.        1.        1.        0.9999999 0.9999999]\n",
      "Top 5 important atoms for molecule 10 (indices): [ 1 29  3 12 24]\n",
      "Importance values: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Molecule 11 (SMILES: *c1ccc(OCCN(CC)c2ccc(C(C#N)=C(C#N)C#N)cc2)c(-c2cc(-c3ccccc3)c3cc(Oc4ccc5nc(*)cc(-c6ccccc6)c5c4)ccc3n2)c1)\n",
      "Actual Tg: 200.00°C, Predicted Tg: 232.94°C\n",
      "Top 5 important atoms (indices): [ 1 29  3 12 24]\n",
      "Importance values: [1. 1. 1. 1. 1.]\n",
      "Top 5 important atoms for molecule 11 (indices): [ 4 13 12 11 10]\n",
      "Importance values: [1.        0.9999999 0.9999999 0.9999999 0.9999999]\n",
      "\n",
      "Molecule 12 (SMILES: *CC(*)c1ccc(CCCC)cc1)\n",
      "Actual Tg: 6.00°C, Predicted Tg: 48.18°C\n",
      "Top 5 important atoms (indices): [ 4 13 12 11 10]\n",
      "Importance values: [1.        0.9999999 0.9999999 0.9999999 0.9999999]\n",
      "Top 5 important atoms for molecule 12 (indices): [52 14 24 23 22]\n",
      "Importance values: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Molecule 13 (SMILES: *c1ccc(/C=C(\\C#N)c2cc(OCCCCCCCC)c(/C(C#N)=C/c3ccc(N(*)c4ccccc4)cc3)cc2OCCCCCCCC)cc1)\n",
      "Actual Tg: 78.00°C, Predicted Tg: 118.09°C\n",
      "Top 5 important atoms (indices): [52 14 24 23 22]\n",
      "Importance values: [1. 1. 1. 1. 1.]\n",
      "Top 5 important atoms for molecule 13 (indices): [20 11 28 27  1]\n",
      "Importance values: [1.        1.        0.9999999 0.9999999 0.9999999]\n",
      "\n",
      "Molecule 14 (SMILES: *CCCCCCCCCOC(=O)c1ccc(-c2ccc(C(=O)O*)cc2)cc1)\n",
      "Actual Tg: 50.50°C, Predicted Tg: 29.21°C\n",
      "Top 5 important atoms (indices): [20 11 28 27  1]\n",
      "Importance values: [1.        1.        0.9999999 0.9999999 0.9999999]\n",
      "Top 5 important atoms for molecule 14 (indices): [ 3  4 14 24 23]\n",
      "Importance values: [1.        1.        1.        0.9999999 0.9999999]\n",
      "\n",
      "Molecule 15 (SMILES: *Oc1c(-c2ccccc2)cc(*)cc1-c1ccc(C(C)(C)C)cc1)\n",
      "Actual Tg: 240.00°C, Predicted Tg: 177.19°C\n",
      "Top 5 important atoms (indices): [ 3  4 14 24 23]\n",
      "Importance values: [1.        1.        1.        0.9999999 0.9999999]\n",
      "Top 5 important atoms for molecule 15 (indices): [24 11 19 33 17]\n",
      "Importance values: [1.        1.        1.        0.9999999 0.9999999]\n",
      "\n",
      "Molecule 16 (SMILES: *CCOCCOP(=O)(/N=N/c1ccc(Oc2ccc(/N=N/P(=O)(O*)OC)cc2)cc1)OC)\n",
      "Actual Tg: 32.00°C, Predicted Tg: 7.05°C\n",
      "Top 5 important atoms (indices): [24 11 19 33 17]\n",
      "Importance values: [1.        1.        1.        0.9999999 0.9999999]\n",
      "Top 5 important atoms for molecule 16 (indices): [13 34 30 41  9]\n",
      "Importance values: [1.        1.        1.        0.9999999 0.9999999]\n",
      "\n",
      "Molecule 17 (SMILES: *CCCCCCCCCN1C(=O)c2ccc(C(c3ccc4c(c3)C(=O)N(*)C4=O)(C(F)(F)F)C(F)(F)F)cc2C1=O)\n",
      "Actual Tg: 135.00°C, Predicted Tg: 135.04°C\n",
      "Top 5 important atoms (indices): [13 34 30 41  9]\n",
      "Importance values: [1.        1.        1.        0.9999999 0.9999999]\n",
      "Top 5 important atoms for molecule 17 (indices): [28 10 13 49 24]\n",
      "Importance values: [1.        1.        0.9999999 0.9999999 0.9999999]\n",
      "\n",
      "Molecule 18 (SMILES: *C(=O)Nc1cccc(NC(=O)c2ccc3c(c2)C(=O)N(c2cc(-c4nc5ccccc5s4)cc(N4C(=O)c5ccc(*)cc5C4=O)c2)C3=O)n1)\n",
      "Actual Tg: 358.00°C, Predicted Tg: 275.07°C\n",
      "Top 5 important atoms (indices): [28 10 13 49 24]\n",
      "Importance values: [1.        1.        0.9999999 0.9999999 0.9999999]\n",
      "Top 5 important atoms for molecule 18 (indices): [50 49 23 22 21]\n",
      "Importance values: [1. 1. 1. 1. 1.]\n",
      "\n",
      "Molecule 19 (SMILES: *c1ccc(C(=O)Oc2ccc3ccc(OC(=O)c4ccc(N5C(=O)CC(SCCOCCSC6CC(=O)N(*)C6=O)C5=O)cc4)cc3c2)cc1)\n",
      "Actual Tg: 160.00°C, Predicted Tg: 133.57°C\n",
      "Top 5 important atoms (indices): [50 49 23 22 21]\n",
      "Importance values: [1. 1. 1. 1. 1.]\n",
      "Top 5 important atoms for molecule 19 (indices): [18 41 10 17 16]\n",
      "Importance values: [1.        0.9999999 0.9999999 0.9999999 0.9999999]\n",
      "\n",
      "Molecule 20 (SMILES: *CCC(=O)Nc1ccc(NC(=O)CCN2C(=O)c3ccc(C(=O)c4ccc5c(c4)C(=O)N(*)C5=O)cc3C2=O)cc1)\n",
      "Actual Tg: 237.00°C, Predicted Tg: 176.62°C\n",
      "Top 5 important atoms (indices): [18 41 10 17 16]\n",
      "Importance values: [1.        0.9999999 0.9999999 0.9999999 0.9999999]\n",
      "\n",
      "Experiment completed successfully!\n",
      "Final R² score: 0.8487\n",
      "Final RMSE: 43.2347\n",
      "\n",
      "Best parameters:\n",
      "  hidden_channels: 32\n",
      "  heads: 4\n",
      "  dropout: 0.11375540844608736\n",
      "  learning_rate: 0.006586289317583112\n",
      "  weight_decay: 5.975027999960295e-06\n",
      "  batch_size: 16\n",
      "  r2_weight: 0.18485445552552704\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import copy\n",
    "\n",
    "# Cross-validation function\n",
    "def cross_validate(data_list, model_class, node_features, edge_features, device, \n",
    "                   n_splits=10, batch_size=32, epochs=100, patience=15):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_list : list\n",
    "        List of PyG Data objects\n",
    "    model_class : class\n",
    "        Model class to instantiate\n",
    "    node_features : int\n",
    "        Number of node features\n",
    "    edge_features : int\n",
    "        Number of edge features\n",
    "    device : torch.device\n",
    "        Device to run the model on\n",
    "    n_splits : int\n",
    "        Number of folds for cross-validation\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    epochs : int\n",
    "        Maximum number of epochs for training\n",
    "    patience : int\n",
    "        Patience for early stopping\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with cross-validation results\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_results = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_r2': [],\n",
    "        'val_rmse': [],\n",
    "        'val_mae': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nPerforming {n_splits}-fold cross-validation...\")\n",
    "    \n",
    "    # Create dataset indices for cross-validation\n",
    "    indices = np.arange(len(data_list))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"\\nFold {fold+1}/{n_splits}\")\n",
    "        \n",
    "        # Split data for this fold\n",
    "        train_data = [data_list[i] for i in train_idx]\n",
    "        val_data = [data_list[i] for i in val_idx]\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = model_class(node_features, edge_features).to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "        \n",
    "        # Initialize early stopping\n",
    "        early_stopping = EarlyStopping(patience=patience)\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_r2s = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Train\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for data in train_loader:\n",
    "                data = data.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, data.y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item() * data.num_graphs\n",
    "            train_loss = total_loss / len(train_loader.dataset)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Validate\n",
    "            val_results = evaluate(model, val_loader, device)\n",
    "            val_loss = val_results['mse']\n",
    "            val_r2 = val_results['r2']\n",
    "            val_losses.append(val_loss)\n",
    "            val_r2s.append(val_r2)\n",
    "            \n",
    "            # Update learning rate scheduler\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Print progress\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, '\n",
    "                      f'Val Loss: {val_loss:.4f}, Val R²: {val_r2:.4f}')\n",
    "            \n",
    "            # Check early stopping\n",
    "            if early_stopping(val_loss, model):\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Load the best model\n",
    "        model.load_state_dict(early_stopping.best_model_state)\n",
    "        \n",
    "        # Final evaluation on validation set\n",
    "        val_results = evaluate(model, val_loader, device)\n",
    "        \n",
    "        # Store results for this fold\n",
    "        fold_results['train_loss'].append(train_losses[-1])\n",
    "        fold_results['val_loss'].append(val_results['mse'])\n",
    "        fold_results['val_r2'].append(val_results['r2'])\n",
    "        fold_results['val_rmse'].append(val_results['rmse'])\n",
    "        fold_results['val_mae'].append(val_results['mae'])\n",
    "        \n",
    "        print(f\"Fold {fold+1} Results - Val MSE: {val_results['mse']:.4f}, \"\n",
    "              f\"Val RMSE: {val_results['rmse']:.4f}, Val R²: {val_results['r2']:.4f}\")\n",
    "    \n",
    "    # Calculate average metrics across folds\n",
    "    for metric in fold_results:\n",
    "        avg_value = np.mean(fold_results[metric])\n",
    "        std_value = np.std(fold_results[metric])\n",
    "        print(f\"Average {metric}: {avg_value:.4f} ± {std_value:.4f}\")\n",
    "    \n",
    "    return fold_results\n",
    "\n",
    "# Custom R² Loss function that optimizes for R² directly\n",
    "class R2Loss(torch.nn.Module):\n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        super(R2Loss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Calculate negative R² (to minimize) as a loss function\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_pred : tensor\n",
    "            Predicted values\n",
    "        y_true : tensor\n",
    "            True values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tensor\n",
    "            Negative R² loss\n",
    "        \"\"\"\n",
    "        y_pred = y_pred.view(-1)\n",
    "        y_true = y_true.view(-1)\n",
    "        \n",
    "        # Calculate mean of true values\n",
    "        y_mean = torch.mean(y_true)\n",
    "        \n",
    "        # Calculate total sum of squares\n",
    "        ss_tot = torch.sum((y_true - y_mean) ** 2)\n",
    "        \n",
    "        # Calculate residual sum of squares\n",
    "        ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "        \n",
    "        # Calculate R²\n",
    "        r2 = 1 - (ss_res / (ss_tot + self.epsilon))\n",
    "        \n",
    "        # Return negative R² for minimization\n",
    "        return -r2\n",
    "\n",
    "# Combined loss function to balance MSE and R² optimization\n",
    "class CombinedLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.7, epsilon=1e-10):\n",
    "        \"\"\"\n",
    "        Combined loss function with MSE and R²\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha : float\n",
    "            Weight for R² loss (1-alpha is weight for MSE)\n",
    "        epsilon : float\n",
    "            Small value to avoid division by zero\n",
    "        \"\"\"\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.mse_loss = torch.nn.MSELoss()\n",
    "        self.r2_loss = R2Loss(epsilon=epsilon)\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        mse = self.mse_loss(y_pred, y_true)\n",
    "        r2 = self.r2_loss(y_pred, y_true)\n",
    "        \n",
    "        # Combine losses (note: lower is better for both)\n",
    "        return (1 - self.alpha) * mse + self.alpha * r2\n",
    "\n",
    "# Modified training function with combined loss\n",
    "def train_with_combined_loss(model, train_loader, optimizer, device, alpha=0.5):\n",
    "    model.train()\n",
    "    total_mse_loss = 0\n",
    "    total_r2_loss = 0\n",
    "    total_combined_loss = 0\n",
    "    \n",
    "    # Create loss functions\n",
    "    mse_criterion = torch.nn.MSELoss()\n",
    "    r2_criterion = R2Loss()\n",
    "    combined_criterion = CombinedLoss(alpha=alpha)\n",
    "    \n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate MSE loss for tracking\n",
    "        mse_loss = mse_criterion(output, data.y)\n",
    "        total_mse_loss += mse_loss.item() * data.num_graphs\n",
    "        \n",
    "        # Calculate R² loss for tracking\n",
    "        r2_loss = r2_criterion(output, data.y)\n",
    "        total_r2_loss += r2_loss.item() * data.num_graphs\n",
    "        \n",
    "        # Use combined loss for optimization\n",
    "        combined_loss = combined_criterion(output, data.y)\n",
    "        combined_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_combined_loss += combined_loss.item() * data.num_graphs\n",
    "    \n",
    "    num_samples = len(train_loader.dataset)\n",
    "    return {\n",
    "        'mse': total_mse_loss / num_samples,\n",
    "        'r2': total_r2_loss / num_samples,\n",
    "        'combined': total_combined_loss / num_samples\n",
    "    }\n",
    "\n",
    "# Modified evaluation function to return R² directly\n",
    "def evaluate_with_r2(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            predictions.extend(output.cpu().numpy())\n",
    "            actual.extend(data.y.cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    actual = np.array(actual)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actual, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predictions)\n",
    "    r2 = r2_score(actual, predictions)\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'actual': actual,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# Hyperparameter tuning with Optuna\n",
    "# Fix for the hyperparameter optimization function\n",
    "def optimize_hyperparameters(data_list, node_features, edge_features, device, n_trials=30):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters using Optuna.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_list : list\n",
    "        List of PyG Data objects\n",
    "    node_features : int\n",
    "        Number of node features\n",
    "    edge_features : int\n",
    "        Number of edge features\n",
    "    device : torch.device\n",
    "        Device to run the model on\n",
    "    n_trials : int\n",
    "        Number of optimization trials\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Best hyperparameters\n",
    "    \"\"\"\n",
    "    print(\"\\nOptimizing hyperparameters with Optuna...\")\n",
    "    \n",
    "    # Split data once for hyperparameter tuning\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        np.arange(len(data_list)), test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_data = [data_list[i] for i in train_idx]\n",
    "    val_data = [data_list[i] for i in val_idx]\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Sample hyperparameters\n",
    "        hidden_channels = trial.suggest_int('hidden_channels', 32, 128, step=16)\n",
    "        heads = trial.suggest_int('heads', 1, 8)\n",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "        r2_weight = trial.suggest_float('r2_weight', 0.0, 1.0)\n",
    "        \n",
    "        # Create loaders with current batch size\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "        \n",
    "        # Create a modified GATTgPredictor with tunable parameters\n",
    "        class TunableGATTgPredictor(torch.nn.Module):\n",
    "            def __init__(self):\n",
    "                super(TunableGATTgPredictor, self).__init__()\n",
    "                \n",
    "                # Graph attention layers - these will provide interpretability\n",
    "                self.conv1 = GATConv(node_features, hidden_channels, heads=heads, dropout=dropout)\n",
    "                self.conv2 = GATConv(hidden_channels*heads, hidden_channels, heads=heads, dropout=dropout)\n",
    "                self.conv3 = GATConv(hidden_channels*heads, hidden_channels, heads=1, dropout=dropout)\n",
    "                \n",
    "                # Batch normalization for stability\n",
    "                self.bn1 = torch.nn.BatchNorm1d(hidden_channels*heads)\n",
    "                self.bn2 = torch.nn.BatchNorm1d(hidden_channels*heads)\n",
    "                self.bn3 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "                \n",
    "                # Fully connected layers for regression\n",
    "                self.fc1 = torch.nn.Linear(hidden_channels, 32)\n",
    "                self.fc2 = torch.nn.Linear(32, 1)\n",
    "                \n",
    "                # Dropout for regularization\n",
    "                self.dropout = torch.nn.Dropout(dropout)\n",
    "                \n",
    "                # For storing attention weights\n",
    "                self.attention_weights = None\n",
    "            \n",
    "            def forward(self, data):\n",
    "                x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "                \n",
    "                # First GAT layer with attention\n",
    "                x1, attention_weights1 = self.conv1(x, edge_index, return_attention_weights=True)\n",
    "                x1 = F.relu(self.bn1(x1))\n",
    "                x1 = self.dropout(x1)\n",
    "                \n",
    "                # Second GAT layer\n",
    "                x2, attention_weights2 = self.conv2(x1, edge_index, return_attention_weights=True)\n",
    "                x2 = F.relu(self.bn2(x2))\n",
    "                x2 = self.dropout(x2)\n",
    "                \n",
    "                # Third GAT layer - final layer for capturing node importance\n",
    "                x3, attention_weights3 = self.conv3(x2, edge_index, return_attention_weights=True)\n",
    "                x3 = F.relu(self.bn3(x3))\n",
    "                \n",
    "                # Store attention weights from the final layer for interpretation\n",
    "                self.attention_weights = attention_weights3\n",
    "                \n",
    "                # Global pooling - aggregate node features for each graph\n",
    "                x = global_mean_pool(x3, batch)\n",
    "                \n",
    "                # Apply fully connected layers\n",
    "                x = F.relu(self.fc1(x))\n",
    "                x = self.dropout(x)\n",
    "                x = self.fc2(x)\n",
    "                \n",
    "                return x.view(-1)\n",
    "            \n",
    "            # Method to get atom-level importance\n",
    "            def get_atom_importance(self, data):\n",
    "                self.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Forward pass to get attention weights\n",
    "                    _ = self(data)\n",
    "                    \n",
    "                    # Extract attention weights\n",
    "                    edge_index, attn_weights = self.attention_weights\n",
    "                    \n",
    "                    # Initialize importance scores for each atom\n",
    "                    num_nodes = data.x.size(0)\n",
    "                    importance = torch.zeros(num_nodes, device=data.x.device)\n",
    "                    \n",
    "                    # Sum attention weights for each node\n",
    "                    for i in range(edge_index.size(1)):\n",
    "                        target_node = edge_index[1, i].item()\n",
    "                        importance[target_node] += attn_weights[i].item()\n",
    "                    \n",
    "                    # Normalize importance scores\n",
    "                    if importance.max() > 0:\n",
    "                        importance = importance / importance.max()\n",
    "                        \n",
    "                    return importance.cpu().numpy()\n",
    "        \n",
    "        # Initialize model with trial parameters\n",
    "        model = TunableGATTgPredictor().to(device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=learning_rate, \n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        # Custom combined loss function\n",
    "        combined_loss = CombinedLoss(alpha=r2_weight)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=False\n",
    "        )\n",
    "        \n",
    "        # Initialize early stopping\n",
    "        early_stopping = EarlyStopping(patience=10)\n",
    "        \n",
    "        # Training loop\n",
    "        max_epochs = 50\n",
    "        for epoch in range(max_epochs):\n",
    "            # Train with combined loss\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for data in train_loader:\n",
    "                data = data.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = combined_loss(output, data.y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * data.num_graphs\n",
    "            \n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            val_predictions = []\n",
    "            val_targets = []\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    data = data.to(device)\n",
    "                    output = model(data)\n",
    "                    val_predictions.extend(output.cpu().numpy())\n",
    "                    val_targets.extend(data.y.cpu().numpy())\n",
    "            \n",
    "            val_predictions = np.array(val_predictions)\n",
    "            val_targets = np.array(val_targets)\n",
    "            val_mse = mean_squared_error(val_targets, val_predictions)\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step(val_mse)\n",
    "            \n",
    "            # Check early stopping\n",
    "            if early_stopping(val_mse, model):\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        model.load_state_dict(early_stopping.best_model_state)\n",
    "        \n",
    "        # Final evaluation\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(device)\n",
    "                output = model(data)\n",
    "                val_predictions.extend(output.cpu().numpy())\n",
    "                val_targets.extend(data.y.cpu().numpy())\n",
    "        \n",
    "        val_predictions = np.array(val_predictions)\n",
    "        val_targets = np.array(val_targets)\n",
    "        val_r2 = r2_score(val_targets, val_predictions)\n",
    "        \n",
    "        # Report R² (higher is better)\n",
    "        return val_r2\n",
    "    \n",
    "    # Create the study and optimize\n",
    "    sampler = TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nBest trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"  Value (R²): {trial.value:.4f}\")\n",
    "    print(\"  Hyperparameters:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    return trial.params\n",
    "\n",
    "# Modified GAT model with additional options for interpretability\n",
    "class EnhancedGATTgPredictor(torch.nn.Module):\n",
    "    def __init__(self, node_features, edge_features, hidden_channels=64, heads=4, \n",
    "                 dropout=0.2, use_gat_v2=False, use_global_attention=False):\n",
    "        super(EnhancedGATTgPredictor, self).__init__()\n",
    "        \n",
    "        self.use_gat_v2 = use_gat_v2\n",
    "        self.use_global_attention = use_global_attention\n",
    "        \n",
    "        # Choose between GAT and GATv2\n",
    "        if use_gat_v2:\n",
    "            # GATv2 has improved attention mechanism\n",
    "            self.conv1 = GATv2Conv(node_features, hidden_channels, heads=heads, dropout=dropout)\n",
    "            self.conv2 = GATv2Conv(hidden_channels*heads, hidden_channels, heads=heads, dropout=dropout)\n",
    "            self.conv3 = GATv2Conv(hidden_channels*heads, hidden_channels, heads=1, dropout=dropout)\n",
    "        else:\n",
    "            # Standard GAT\n",
    "            self.conv1 = GATConv(node_features, hidden_channels, heads=heads, dropout=dropout)\n",
    "            self.conv2 = GATConv(hidden_channels*heads, hidden_channels, heads=heads, dropout=dropout)\n",
    "            self.conv3 = GATConv(hidden_channels*heads, hidden_channels, heads=1, dropout=dropout)\n",
    "        \n",
    "        # Batch normalization for stability\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels*heads)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels*heads)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        # Optional global attention pooling\n",
    "        if use_global_attention:\n",
    "            self.global_attention = GlobalAttention(\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(hidden_channels // 2, 1)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Fully connected layers for regression\n",
    "        self.fc1 = torch.nn.Linear(hidden_channels, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 1)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # For storing attention weights\n",
    "        self.attention_weights = None\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # First GAT layer with attention\n",
    "        if self.use_gat_v2:\n",
    "            x1 = self.conv1(x, edge_index)\n",
    "            self.attention_weights = None  # GATv2Conv doesn't return attention weights by default\n",
    "        else:\n",
    "            x1, attention_weights1 = self.conv1(x, edge_index, return_attention_weights=True)\n",
    "            self.attention_weights = attention_weights1\n",
    "            \n",
    "        x1 = F.relu(self.bn1(x1))\n",
    "        x1 = self.dropout(x1)\n",
    "        \n",
    "        # Second GAT layer\n",
    "        if self.use_gat_v2:\n",
    "            x2 = self.conv2(x1, edge_index)\n",
    "        else:\n",
    "            x2, attention_weights2 = self.conv2(x1, edge_index, return_attention_weights=True)\n",
    "            \n",
    "        x2 = F.relu(self.bn2(x2))\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        # Third GAT layer - final layer for capturing node importance\n",
    "        if self.use_gat_v2:\n",
    "            x3 = self.conv3(x2, edge_index)\n",
    "        else:\n",
    "            x3, attention_weights3 = self.conv3(x2, edge_index, return_attention_weights=True)\n",
    "            self.attention_weights = attention_weights3\n",
    "            \n",
    "        x3 = F.relu(self.bn3(x3))\n",
    "        \n",
    "        # Global pooling - aggregate node features for each graph\n",
    "        if self.use_global_attention:\n",
    "            x = self.global_attention(x3, batch)\n",
    "        else:\n",
    "            x = global_mean_pool(x3, batch)\n",
    "        \n",
    "        # Apply fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x.view(-1)\n",
    "    \n",
    "    # Method to get atom-level importance\n",
    "    def get_atom_importance(self, data):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass to get attention weights\n",
    "            _ = self(data)\n",
    "            \n",
    "            if self.use_gat_v2:\n",
    "                # For GATv2, we need a different approach to get importance\n",
    "                # Run a forward pass and use gradients with respect to node features\n",
    "                self.train()  # temporarily set to train mode to compute gradients\n",
    "                \n",
    "                # Create a copy of node features that requires gradients\n",
    "                x = data.x.clone().detach().to(data.x.device).requires_grad_(True)\n",
    "                \n",
    "                # Forward pass with the copied features\n",
    "                data_copy = copy.copy(data)\n",
    "                data_copy.x = x\n",
    "                out = self(data_copy)\n",
    "                \n",
    "                # Compute gradients\n",
    "                out.backward()\n",
    "                \n",
    "                # Use gradient magnitudes as importance scores\n",
    "                importance = torch.sum(torch.abs(x.grad), dim=1)\n",
    "                \n",
    "                # Normalize importance scores\n",
    "                if importance.max() > 0:\n",
    "                    importance = importance / importance.max()\n",
    "                \n",
    "                self.eval()  # set back to eval mode\n",
    "                return importance.cpu().numpy()\n",
    "            \n",
    "            # For standard GAT, use attention weights\n",
    "            if self.attention_weights is None:\n",
    "                # Fallback if no attention weights\n",
    "                num_nodes = data.x.size(0)\n",
    "                return np.ones(num_nodes) / num_nodes\n",
    "                \n",
    "            # Extract attention weights\n",
    "            edge_index, attn_weights = self.attention_weights\n",
    "            \n",
    "            # Initialize importance scores for each atom\n",
    "            num_nodes = data.x.size(0)\n",
    "            importance = torch.zeros(num_nodes, device=data.x.device)\n",
    "            \n",
    "            # Sum attention weights for each node\n",
    "            for i in range(edge_index.size(1)):\n",
    "                target_node = edge_index[1, i].item()\n",
    "                importance[target_node] += attn_weights[i].item()\n",
    "            \n",
    "            # Normalize importance scores\n",
    "            if importance.max() > 0:\n",
    "                importance = importance / importance.max()\n",
    "                \n",
    "            return importance.cpu().numpy()\n",
    "\n",
    "# Function to run an experiment with optimized model and cross-validation\n",
    "def run_optimized_experiment(data_list, best_params, node_features, edge_features, device, \n",
    "                            test_size=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Run a complete experiment with the optimized model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_list : list\n",
    "        List of PyG Data objects\n",
    "    best_params : dict\n",
    "        Best hyperparameters from optimization\n",
    "    node_features : int\n",
    "        Number of node features\n",
    "    edge_features : int\n",
    "        Number of edge features\n",
    "    device : torch.device\n",
    "        Device to run the model on\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Experiment results\n",
    "    \"\"\"\n",
    "    # Set reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create train/test split\n",
    "    train_data, test_data = train_test_split(data_list, test_size=test_size, random_state=seed)\n",
    "    \n",
    "    print(f\"\\nRunning final experiment with optimized parameters...\")\n",
    "    print(f\"Training set: {len(train_data)}, Test set: {len(test_data)}\")\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    hidden_channels = best_params.get('hidden_channels', 64)\n",
    "    heads = best_params.get('heads', 4)\n",
    "    dropout = best_params.get('dropout', 0.2)\n",
    "    learning_rate = best_params.get('learning_rate', 0.001)\n",
    "    weight_decay = best_params.get('weight_decay', 1e-4)\n",
    "    batch_size = best_params.get('batch_size', 64)\n",
    "    r2_weight = best_params.get('r2_weight', 0.5)\n",
    "    \n",
    "    # Initialize model with optimized parameters\n",
    "    model = EnhancedGATTgPredictor(\n",
    "        node_features, edge_features,\n",
    "        hidden_channels=hidden_channels,\n",
    "        heads=heads,\n",
    "        dropout=dropout,\n",
    "        use_gat_v2=False,  # Change to True to try GATv2\n",
    "        use_global_attention=False  # Change to True to try global attention\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create optimized dataloader\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "    \n",
    "    # Create optimizer with optimized parameters\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=15)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 200\n",
    "    train_losses = []\n",
    "    train_r2s = []\n",
    "    \n",
    "    print(\"\\nTraining optimized model...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train with combined loss\n",
    "        train_results = train_with_combined_loss(\n",
    "            model, train_loader, optimizer, device, alpha=r2_weight\n",
    "        )\n",
    "        train_losses.append(train_results['mse'])\n",
    "        \n",
    "        # Validate on training set for tracking\n",
    "        train_eval = evaluate_with_r2(model, train_loader, device)\n",
    "        train_r2s.append(train_eval['r2'])\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_results[\"mse\"]:.4f}, '\n",
    "                  f'Train R²: {train_eval[\"r2\"]:.4f}')\n",
    "        \n",
    "        # Update scheduler based on MSE\n",
    "        scheduler.step(train_results['mse'])\n",
    "        \n",
    "        # Check early stopping\n",
    "        if early_stopping(train_results['mse'], model):\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(early_stopping.best_model_state)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_results = evaluate_with_r2(model, test_loader, device)\n",
    "    print(\"\\nTest Results with Optimized Model:\")\n",
    "    print(f\"MSE: {test_results['mse']:.4f}\")\n",
    "    print(f\"RMSE: {test_results['rmse']:.4f}\")\n",
    "    print(f\"MAE: {test_results['mae']:.4f}\")\n",
    "    print(f\"R²: {test_results['r2']:.4f}\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss (MSE)')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_r2s, label='Train R²')\n",
    "    plt.axhline(y=test_results['r2'], color='r', linestyle='--', label='Test R²')\n",
    "    plt.title('R² Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{plot_dir_name}optimized_training_curves.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot predictions vs actual\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(test_results['actual'], test_results['predictions'], alpha=0.5)\n",
    "    \n",
    "    # Add identity line\n",
    "    min_val = min(min(test_results['actual']), min(test_results['predictions']))\n",
    "    max_val = max(max(test_results['actual']), max(test_results['predictions']))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    plt.xlabel('Actual Tg (°C)')\n",
    "    plt.ylabel('Predicted Tg (°C)')\n",
    "    plt.title(f'Optimized GAT Model: Actual vs Predicted Tg (R² = {test_results[\"r2\"]:.4f})')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(f'{plot_dir_name}optimized_prediction_results.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Analyze model interpretation\n",
    "    print(\"\\nAnalyzing model interpretation with attention weights...\")\n",
    "    analyze_model_interpretation(model, test_loader, device, num_examples=20)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'test_results': test_results,\n",
    "        'best_params': best_params\n",
    "    }\n",
    "\n",
    "# Main function to run the complete workflow\n",
    "def run_complete_workflow(data, smiles_col='SMILES', target_col='tg'):\n",
    "    \"\"\"\n",
    "    Run the complete workflow from data preparation to optimized model training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        DataFrame with molecule data\n",
    "    smiles_col : str\n",
    "        Column name for SMILES strings\n",
    "    target_col : str\n",
    "        Column name for target values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Results from the experiment\n",
    "    \"\"\"\n",
    "    # Analyze data\n",
    "    data = analyze_data(data)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    print(\"\\nPreparing dataset...\")\n",
    "    data_list, valid_indices = prepare_dataset(data, smiles_col=smiles_col, target_col=target_col)\n",
    "    print(f\"Valid molecules processed: {len(data_list)} out of {len(data)}\")\n",
    "    \n",
    "    # Extract node/edge features\n",
    "    if len(data_list) > 0:\n",
    "        node_features = data_list[0].x.shape[1]\n",
    "        edge_features = data_list[0].edge_attr.shape[1] if data_list[0].edge_attr.shape[0] > 0 else 0\n",
    "        print(f\"Node features: {node_features}, Edge features: {edge_features}\")\n",
    "    else:\n",
    "        print(\"Error: No valid molecules processed.\")\n",
    "        return None\n",
    "    \n",
    "    # Check device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 1. Perform cross-validation with baseline model\n",
    "    print(\"\\nPerforming cross-validation with baseline model...\")\n",
    "    cv_results = cross_validate(\n",
    "        data_list, GATTgPredictor, node_features, edge_features, device, \n",
    "        n_splits=10, batch_size=64, epochs=100\n",
    "    )\n",
    "    \n",
    "    # 2. Hyperparameter optimization\n",
    "    print(\"\\nOptimizing hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(\n",
    "        data_list, node_features, edge_features, device, n_trials=5\n",
    "    )\n",
    "    \n",
    "    # 3. Train optimized model\n",
    "    print(\"\\nTraining final model with optimized parameters...\")\n",
    "    results = run_optimized_experiment(\n",
    "        data_list, best_params, node_features, edge_features, device\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Code to integrate the new functionality into the main workflow\n",
    "if __name__ == \"__main__\":\n",
    "    # Set seeds for reproducibility\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Load data\n",
    "    DATA_PATH = \"./data/D1/tg_raw.csv\"\n",
    "    data = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Dataset shape: {data.shape}\")\n",
    "    print(data.head())\n",
    "    \n",
    "    # Run the complete workflow\n",
    "    results = run_complete_workflow(data)\n",
    "    \n",
    "    # Print final results\n",
    "    if results:\n",
    "        print(\"\\nExperiment completed successfully!\")\n",
    "        print(f\"Final R² score: {results['test_results']['r2']:.4f}\")\n",
    "        print(f\"Final RMSE: {results['test_results']['rmse']:.4f}\")\n",
    "        print(\"\\nBest parameters:\")\n",
    "        for param, value in results['best_params'].items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "    else:\n",
    "        print(\"Experiment failed to complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adversaGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
